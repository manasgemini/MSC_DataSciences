{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVlxxITFfNSA"
   },
   "source": [
    "# **Text Analytics and Preprocessing**\n",
    "# This week we will learn how to process text data using different text processing methods including text parsing, tokenization, Bag of Words, TF, IDF, TF-IDF representation, sentiment analysis etc.\n",
    "# We start with a few exercises on each topic and then the students will complete a set of related exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFp12wpHgISI"
   },
   "source": [
    "# **Example: Text Tokenization**\n",
    "**Description:** Tokenization is the process of breaking down a text into individual words or tokens. In this example, we demonstrate how to tokenize a given text using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPhWG4JSiSjp",
    "outputId": "b6cc9ff9-dfc7-4787-88cb-4286668c6539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', '(NLP)', 'is', 'a', 'subfield', 'of', 'linguistics,', 'computer', 'science,', 'and', 'artificial', 'intelligence.']\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = \"Natural language processing (NLP) is a subfield of linguistics, \" \\\n",
    "       \"computer science, and artificial intelligence.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Print the tokens\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zB6eGhGGivC8"
   },
   "source": [
    "# **Example: Removing Stopwords**\n",
    "**Description:** Stopwords are common words like \"the\", \"is\", \"and\", etc., that do not carry significant meaning in text analysis. In this example scenario, students are tasked with removing stopwords from a given text to preprocess it for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "geTkauhQifaz",
    "outputId": "f950af5c-194a-4492-f253-d1b4fd6a7822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'analytics', 'process', 'analyzing', 'unstructured', 'text', 'data', 'extract', 'meaningful', 'insights.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pandeym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example text\n",
    "text = \"Text analytics is the process of analyzing unstructured text data to extract meaningful insights.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = text.split()\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Print the filtered tokens\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs8cDx0bhRjY"
   },
   "source": [
    "# **Text Representation and Feature Extraction**\n",
    "# **Example: Implementing Bag-of-Words Representation**\n",
    "**Description:** Bag-of-Words (BoW) representation is a simple technique for converting text data into numerical form. In this example, we show how to implement a BoW representation for a corpus of text documents using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mEZEo-pTi-Wu",
    "outputId": "631a8e57-2f68-4d8c-c52d-6e6d3687f8a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "Bag-of-Words Matrix:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the Bag-of-Words matrix\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U60NvifojQ-C"
   },
   "source": [
    "# **Example: Implementing TF-IDF Representation**\n",
    "**Description:** TF-IDF (Term Frequency-Inverse Document Frequency) is a popular technique for text representation that considers both the frequency of a term in a document and its rarity across all documents. In this example scenario, students are introduced to TF-IDF representation and guided to implement it for a given corpus of text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFFJVbNxjVRA",
    "outputId": "d77b52f2-14ae-4594-b046-6bb12ba6b70b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Example corpus\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "# Create TfidfVectorizer object\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the TF-IDF matrix\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38-8icbQhone"
   },
   "source": [
    "# **Text Classification**\n",
    "# **Example: Text Classification using Naive Bayes Classifier**\n",
    "**Description:** Text classification involves categorizing text documents into predefined classes or categories. In this example, we demonstrate how to perform text classification using a Naive Bayes classifier, a popular algorithm for text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RJTExMr9hG2d",
    "outputId": "a97f10d2-c18b-41c6-b92f-f505653ac711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.97      0.60      0.74       319\n",
      "         comp.graphics       0.96      0.89      0.92       389\n",
      "               sci.med       0.97      0.81      0.88       396\n",
      "soc.religion.christian       0.65      0.99      0.78       398\n",
      "\n",
      "              accuracy                           0.83      1502\n",
      "             macro avg       0.89      0.82      0.83      1502\n",
      "          weighted avg       0.88      0.83      0.84      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load 20newsgroups dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(twenty_train.data)\n",
    "y_train = twenty_train.target\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "X_test = vectorizer.transform(twenty_test.data)\n",
    "y_test = twenty_test.target\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred, target_names=twenty_test.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FawIXthJm8Ng"
   },
   "source": [
    "# **Example: Text Classification using Support Vector Machine (SVM)**\n",
    "**Description:** Support Vector Machine (SVM) is another commonly used algorithm for text classification tasks. In this example scenario, students are introduced to a dataset containing text documents belonging to different categories and tasked with training an SVM classifier to classify the documents into their respective categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "YqC6GY4Pm2iq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.96      0.83      0.89       319\n",
      "         comp.graphics       0.90      0.96      0.93       389\n",
      "               sci.med       0.94      0.91      0.93       396\n",
      "soc.religion.christian       0.89      0.96      0.93       398\n",
      "\n",
      "              accuracy                           0.92      1502\n",
      "             macro avg       0.93      0.92      0.92      1502\n",
      "          weighted avg       0.92      0.92      0.92      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load 20newsgroups dataset\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(twenty_train.data)\n",
    "y_train = twenty_train.target\n",
    "\n",
    "# Train SVM classifier\n",
    "clf = SVC(kernel='linear')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the classifier\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "X_test = vectorizer.transform(twenty_test.data)\n",
    "y_test = twenty_test.target\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred, target_names=twenty_test.target_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VNrtAzKeghN"
   },
   "source": [
    "# **Example: Sentiment Analysis on product reviews**\n",
    "This example creates a small review database that contains reviews on a certain product either positive, negative or neutral. We use machine learning to train and test the model using this data (after performing basic preprocessing, and feature extraction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "frABpM9T-0Y3",
    "outputId": "63da2f31-4404-442a-ca52-5e7b6302b46f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00       1.0\n",
      "    Positive       0.00      0.00      0.00       0.0\n",
      "\n",
      "    accuracy                           0.00       1.0\n",
      "   macro avg       0.00      0.00      0.00       1.0\n",
      "weighted avg       0.00      0.00      0.00       1.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    \"Review\": [\n",
    "        \"This product is amazing! I love it.\",\n",
    "        \"The quality of the product is poor. Disappointed.\",\n",
    "        \"It meets my expectations. Good value for money.\",\n",
    "        \"I don't like it. Waste of money.\",\n",
    "        \"Neutral opinion about the product.\"\n",
    "    ],\n",
    "    \"Sentiment\": [\"Positive\", \"Negative\", \"Positive\", \"Negative\", \"Neutral\"]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Review'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Text preprocessing and feature extraction (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "train_features = vectorizer.fit_transform(train_texts)\n",
    "test_features = vectorizer.transform(test_texts)\n",
    "\n",
    "# Model training (Logistic Regression)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Model evaluation\n",
    "predictions = model.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(test_labels, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEXS3q-EoXJI"
   },
   "source": [
    "# **---------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOhmvcm2oehd"
   },
   "source": [
    "# **Exercises**\n",
    "# Please complete the following exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81V95BnSomD2"
   },
   "source": [
    "# **Text Tokenization Exercise:**\n",
    "# Scenario:\n",
    "You have a dataset containing customer feedback for a restaurant. Each feedback entry is a string representing a customer's comment. Your task is to tokenize each feedback into individual words to analyze the most commonly mentioned aspects of the restaurant.\n",
    "# Dataset: Customer Feedback for a Restaurant\n",
    "\n",
    "1. \"The food was delicious and the service was excellent.\"\n",
    "2. \"We had a great time dining at this restaurant.\"\n",
    "3. \"The ambiance was cozy and welcoming.\"\n",
    "4. \"The staff was friendly but the food took too long to arrive.\"\n",
    "5. \"I didn't enjoy the meal as much as I had hoped.\"\n",
    "6. \"The desserts were divine!\"\n",
    "7. \"The portions were generous, and the prices were reasonable.\"\n",
    "8. \"The restaurant was crowded, and we had to wait for a table.\"\n",
    "9. \"The presentation of the dishes was beautiful.\"\n",
    "10. \"Overall, it was a pleasant dining experience.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'food', 'was', 'delicious', 'and', 'the', 'service', 'was', 'excellent.']\n",
      "['We', 'had', 'a', 'great', 'time', 'dining', 'at', 'this', 'restaurant.']\n",
      "['The', 'ambiance', 'was', 'cozy', 'and', 'welcoming.']\n",
      "['The', 'staff', 'was', 'friendly', 'but', 'the', 'food', 'took', 'too', 'long', 'to', 'arrive.']\n",
      "['I', \"didn't\", 'enjoy', 'the', 'meal', 'as', 'much', 'as', 'I', 'had', 'hoped.']\n",
      "['The', 'desserts', 'were', 'divine!']\n",
      "['The', 'portions', 'were', 'generous,', 'and', 'the', 'prices', 'were', 'reasonable.']\n",
      "['The', 'restaurant', 'was', 'crowded,', 'and', 'we', 'had', 'to', 'wait', 'for', 'a', 'table.']\n",
      "['The', 'presentation', 'of', 'the', 'dishes', 'was', 'beautiful.']\n",
      "['Overall,', 'it', 'was', 'a', 'pleasant', 'dining', 'experience.']\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "feedbacks = [\"The food was delicious and the service was excellent.\",\n",
    "\"We had a great time dining at this restaurant.\",\n",
    "\"The ambiance was cozy and welcoming.\",\n",
    "\"The staff was friendly but the food took too long to arrive.\",\n",
    "\"I didn't enjoy the meal as much as I had hoped.\",\n",
    "\"The desserts were divine!\",\n",
    "\"The portions were generous, and the prices were reasonable.\",\n",
    "\"The restaurant was crowded, and we had to wait for a table.\",\n",
    "\"The presentation of the dishes was beautiful.\",\n",
    "\"Overall, it was a pleasant dining experience.\"]\n",
    "\n",
    "for text in feedbacks:\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Print the tokens\n",
    "    print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA90I6XEo_FL"
   },
   "source": [
    "# **Removing Stopwords Exercise:**\n",
    "# Scenario:\n",
    "You are analyzing movie reviews for a film festival. The dataset consists of reviews from various movie critics. Before sentiment analysis, you need to preprocess the text data by removing stopwords to focus on the meaningful content of the reviews.\n",
    "# Dataset: Movie Reviews for Film Festival\n",
    "\n",
    "1. \"The cinematography in this film is absolutely stunning, and the acting performances are top-notch.\"\n",
    "2. \"I found the plot to be quite predictable, but the visual effects were impressive.\"\n",
    "3. \"The dialogue felt natural, and the chemistry between the lead actors was palpable.\"\n",
    "4. \"This movie kept me on the edge of my seat from start to finish. A thrilling experience!\"\n",
    "5. \"I was disappointed by the lack of character development. The story felt shallow and unconvincing.\"\n",
    "6. \"The soundtrack perfectly complemented the mood of the film. A standout aspect for me.\"\n",
    "7. \"The pacing of the movie was uneven, making it difficult to fully engage with the story.\"\n",
    "8. \"Despite its flaws, this film offers a unique perspective on a familiar genre.\"\n",
    "9. \"The ending was satisfying and left me contemplating its deeper meaning long after the credits rolled.\"\n",
    "10. \"Overall, a thought-provoking film that stays with you.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: 'The cinematography in this film is absolutely stunning, and the acting performances are top-notch.'\n",
      "Tokens without stop words:  ['cinematography', 'film', 'absolutely', 'stunning,', 'acting', 'performances', 'top-notch.']\n",
      "Review: 'I found the plot to be quite predictable, but the visual effects were impressive.'\n",
      "Tokens without stop words:  ['found', 'plot', 'quite', 'predictable,', 'visual', 'effects', 'impressive.']\n",
      "Review: 'The dialogue felt natural, and the chemistry between the lead actors was palpable.'\n",
      "Tokens without stop words:  ['dialogue', 'felt', 'natural,', 'chemistry', 'lead', 'actors', 'palpable.']\n",
      "Review: 'This movie kept me on the edge of my seat from start to finish. A thrilling experience!'\n",
      "Tokens without stop words:  ['movie', 'kept', 'edge', 'seat', 'start', 'finish.', 'thrilling', 'experience!']\n",
      "Review: 'I was disappointed by the lack of character development. The story felt shallow and unconvincing.'\n",
      "Tokens without stop words:  ['disappointed', 'lack', 'character', 'development.', 'story', 'felt', 'shallow', 'unconvincing.']\n",
      "Review: 'The soundtrack perfectly complemented the mood of the film. A standout aspect for me.'\n",
      "Tokens without stop words:  ['soundtrack', 'perfectly', 'complemented', 'mood', 'film.', 'standout', 'aspect', 'me.']\n",
      "Review: 'The pacing of the movie was uneven, making it difficult to fully engage with the story.'\n",
      "Tokens without stop words:  ['pacing', 'movie', 'uneven,', 'making', 'difficult', 'fully', 'engage', 'story.']\n",
      "Review: 'Despite its flaws, this film offers a unique perspective on a familiar genre.'\n",
      "Tokens without stop words:  ['Despite', 'flaws,', 'film', 'offers', 'unique', 'perspective', 'familiar', 'genre.']\n",
      "Review: 'The ending was satisfying and left me contemplating its deeper meaning long after the credits rolled.'\n",
      "Tokens without stop words:  ['ending', 'satisfying', 'left', 'contemplating', 'deeper', 'meaning', 'long', 'credits', 'rolled.']\n",
      "Review: 'Overall, a thought-provoking film that stays with you.'\n",
      "Tokens without stop words:  ['Overall,', 'thought-provoking', 'film', 'stays', 'you.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pandeym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Example text\n",
    "reviews = [\n",
    "    \"The cinematography in this film is absolutely stunning, and the acting performances are top-notch.\",\n",
    "    \"I found the plot to be quite predictable, but the visual effects were impressive.\",\n",
    "    \"The dialogue felt natural, and the chemistry between the lead actors was palpable.\",\n",
    "    \"This movie kept me on the edge of my seat from start to finish. A thrilling experience!\",\n",
    "    \"I was disappointed by the lack of character development. The story felt shallow and unconvincing.\",\n",
    "    \"The soundtrack perfectly complemented the mood of the film. A standout aspect for me.\",\n",
    "    \"The pacing of the movie was uneven, making it difficult to fully engage with the story.\",\n",
    "    \"Despite its flaws, this film offers a unique perspective on a familiar genre.\",\n",
    "    \"The ending was satisfying and left me contemplating its deeper meaning long after the credits rolled.\",\n",
    "    \"Overall, a thought-provoking film that stays with you.\"\n",
    "]\n",
    "\n",
    "for text in reviews:\n",
    "    print(\"Review: '\"+text+\"'\")\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Print the filtered tokens\n",
    "    print(\"Tokens without stop words: \",filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25rLVxLipYTB"
   },
   "source": [
    "# **Bag-of-Words Representation Exercise:**\n",
    "# Scenario:\n",
    "You are working on a sentiment analysis project for product reviews. You have a dataset containing reviews for electronic gadgets. Convert the dataset into a Bag-of-Words representation, treating each unique word in the reviews as a feature, to train a sentiment classifier.\n",
    "# Dataset: Product Reviews for Electronic Gadgets\n",
    "\n",
    "1. \"The smartphone has a sleek design and great performance.\"\n",
    "2. \"The battery life of this tablet is disappointing.\"\n",
    "3. \"I love the camera quality of this digital camera.\"\n",
    "4. \"The sound quality of these headphones is amazing.\"\n",
    "5. \"The laptop froze multiple times within the first week of use.\"\n",
    "6. \"This smartwatch is easy to use and has useful features.\"\n",
    "7. \"The touchscreen of this e-reader is unresponsive at times.\"\n",
    "8. \"The gaming console heats up quickly during extended use.\"\n",
    "9. \"The voice recognition feature of this smart speaker is impressive.\"\n",
    "10. \"The software update improved the functionality of this fitness tracker.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['amazing' 'and' 'at' 'battery' 'camera' 'console' 'design' 'digital'\n",
      " 'disappointing' 'during' 'easy' 'extended' 'feature' 'features' 'first'\n",
      " 'fitness' 'froze' 'functionality' 'gaming' 'great' 'has' 'headphones'\n",
      " 'heats' 'impressive' 'improved' 'is' 'laptop' 'life' 'love' 'multiple'\n",
      " 'of' 'performance' 'quality' 'quickly' 'reader' 'recognition' 'sleek'\n",
      " 'smart' 'smartphone' 'smartwatch' 'software' 'sound' 'speaker' 'tablet'\n",
      " 'the' 'these' 'this' 'times' 'to' 'touchscreen' 'tracker' 'unresponsive'\n",
      " 'up' 'update' 'use' 'useful' 'voice' 'week' 'within']\n",
      "Bag-of-Words Matrix:\n",
      "[[0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 2 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0\n",
      "  0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 2 0 0 1 0 0 0 0 0 0 1 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 1\n",
      "  0 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 2 0 1 0 0 0 1 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example corpus\n",
    "reviews = [\n",
    "    \"The smartphone has a sleek design and great performance.\",\n",
    "    \"The battery life of this tablet is disappointing.\",\n",
    "    \"I love the camera quality of this digital camera.\",\n",
    "    \"The sound quality of these headphones is amazing.\",\n",
    "    \"The laptop froze multiple times within the first week of use.\",\n",
    "    \"This smartwatch is easy to use and has useful features.\",\n",
    "    \"The touchscreen of this e-reader is unresponsive at times.\",\n",
    "    \"The gaming console heats up quickly during extended use.\",\n",
    "    \"The voice recognition feature of this smart speaker is impressive.\",\n",
    "    \"The software update improved the functionality of this fitness tracker.\"\n",
    "]\n",
    "\n",
    "# Create CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X = vectorizer.fit_transform(reviews)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "\n",
    "# Print the Bag-of-Words matrix\n",
    "print(\"Bag-of-Words Matrix:\")\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seXI7QBOpq6J"
   },
   "source": [
    "# **TF-IDF Implementation Exercise:**\n",
    "# Scenario:\n",
    "You are working on a project to classify news articles into different categories. Implement TF-IDF representation for a corpus of news articles to prepare them for classification.\n",
    "# Dataset: News Articles for Classification\n",
    "\n",
    "Category: Politics\n",
    "1. \"The government announced new policies to address economic challenges.\"\n",
    "2. \"Opposition leaders criticized the proposed budget for its lack of transparency.\"\n",
    "3. \"Political tensions rise as the election date approaches.\"\n",
    "\n",
    "Category: Technology\n",
    "4. \"Apple unveils its latest iPhone model with advanced features.\"\n",
    "5. \"Google launches a new artificial intelligence research lab.\"\n",
    "6. \"Microsoft announces plans to acquire a leading cybersecurity firm.\"\n",
    "\n",
    "Category: Sports\n",
    "7. \"The home team secures a decisive victory in the championship game.\"\n",
    "8. \"A star athlete signs a record-breaking contract with a professional team.\"\n",
    "9. \"An underdog team surprises everyone by advancing to the finals.\"\n",
    "\n",
    "Category: Health\n",
    "10. \"Researchers discover a potential breakthrough in cancer treatment.\"\n",
    "11. \"Health officials warn of a spike in flu cases during the winter season.\"\n",
    "12. \"The benefits of regular exercise on mental health are highlighted in a new study.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     acquire   address  advanced  advancing        an  announced  announces  \\\n",
      "0   0.000000  0.367145    0.0000   0.000000  0.000000   0.367145   0.000000   \n",
      "1   0.000000  0.000000    0.0000   0.000000  0.000000   0.000000   0.000000   \n",
      "2   0.000000  0.000000    0.0000   0.000000  0.000000   0.000000   0.000000   \n",
      "3   0.000000  0.000000    0.3435   0.000000  0.000000   0.000000   0.000000   \n",
      "4   0.000000  0.000000    0.0000   0.000000  0.000000   0.000000   0.000000   \n",
      "5   0.363324  0.000000    0.0000   0.000000  0.000000   0.000000   0.363324   \n",
      "6   0.000000  0.000000    0.0000   0.000000  0.000000   0.000000   0.000000   \n",
      "7   0.000000  0.000000    0.0000   0.000000  0.000000   0.000000   0.000000   \n",
      "8   0.000000  0.000000    0.0000   0.344651  0.344651   0.000000   0.000000   \n",
      "9   0.000000  0.000000    0.0000   0.000000  0.000000   0.000000   0.000000   \n",
      "10  0.000000  0.000000    0.0000   0.000000  0.000000   0.000000   0.000000   \n",
      "11  0.000000  0.000000    0.0000   0.000000  0.000000   0.000000   0.000000   \n",
      "\n",
      "     apple  approaches      are  ...       the        to  transparency  \\\n",
      "0   0.0000    0.000000  0.00000  ...  0.189915  0.278530      0.000000   \n",
      "1   0.0000    0.000000  0.00000  ...  0.167118  0.000000      0.323074   \n",
      "2   0.0000    0.370941  0.00000  ...  0.191878  0.000000      0.000000   \n",
      "3   0.3435    0.000000  0.00000  ...  0.000000  0.000000      0.000000   \n",
      "4   0.0000    0.000000  0.00000  ...  0.000000  0.000000      0.000000   \n",
      "5   0.0000    0.000000  0.00000  ...  0.000000  0.275631      0.000000   \n",
      "6   0.0000    0.000000  0.00000  ...  0.363290  0.000000      0.000000   \n",
      "7   0.0000    0.000000  0.00000  ...  0.000000  0.000000      0.000000   \n",
      "8   0.0000    0.000000  0.00000  ...  0.178279  0.261465      0.000000   \n",
      "9   0.0000    0.000000  0.00000  ...  0.000000  0.000000      0.000000   \n",
      "10  0.0000    0.000000  0.00000  ...  0.163215  0.000000      0.000000   \n",
      "11  0.0000    0.000000  0.30686  ...  0.158731  0.000000      0.000000   \n",
      "\n",
      "    treatment  underdog  unveils   victory      warn    winter      with  \n",
      "0    0.000000  0.000000   0.0000  0.000000  0.000000  0.000000  0.000000  \n",
      "1    0.000000  0.000000   0.0000  0.000000  0.000000  0.000000  0.000000  \n",
      "2    0.000000  0.000000   0.0000  0.000000  0.000000  0.000000  0.000000  \n",
      "3    0.000000  0.000000   0.3435  0.000000  0.000000  0.000000  0.295002  \n",
      "4    0.000000  0.000000   0.0000  0.000000  0.000000  0.000000  0.000000  \n",
      "5    0.000000  0.000000   0.0000  0.000000  0.000000  0.000000  0.000000  \n",
      "6    0.000000  0.000000   0.0000  0.351159  0.000000  0.000000  0.000000  \n",
      "7    0.000000  0.000000   0.0000  0.000000  0.000000  0.000000  0.297863  \n",
      "8    0.000000  0.344651   0.0000  0.000000  0.000000  0.000000  0.000000  \n",
      "9    0.393333  0.000000   0.0000  0.000000  0.000000  0.000000  0.000000  \n",
      "10   0.000000  0.000000   0.0000  0.000000  0.315529  0.315529  0.000000  \n",
      "11   0.000000  0.000000   0.0000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[12 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Corpus of news articles (as a list of strings)\n",
    "corpus = [\n",
    "    \"The government announced new policies to address economic challenges.\",\n",
    "    \"Opposition leaders criticized the proposed budget for its lack of transparency.\",\n",
    "    \"Political tensions rise as the election date approaches.\",\n",
    "    \"Apple unveils its latest iPhone model with advanced features.\",\n",
    "    \"Google launches a new artificial intelligence research lab.\",\n",
    "    \"Microsoft announces plans to acquire a leading cybersecurity firm.\",\n",
    "    \"The home team secures a decisive victory in the championship game.\",\n",
    "    \"A star athlete signs a record-breaking contract with a professional team.\",\n",
    "    \"An underdog team surprises everyone by advancing to the finals.\",\n",
    "    \"Researchers discover a potential breakthrough in cancer treatment.\",\n",
    "    \"Health officials warn of a spike in flu cases during the winter season.\",\n",
    "    \"The benefits of regular exercise on mental health are highlighted in a new study.\"\n",
    "]\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the corpus to get the TF-IDF matrix\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array\n",
    "dense_matrix = X.todense()\n",
    "\n",
    "# Create a DataFrame to view the TF-IDF scores for each term in each document\n",
    "import pandas as pd\n",
    "df_tfidf = pd.DataFrame(dense_matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Show the TF-IDF matrix\n",
    "print(df_tfidf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWUeXxp7p_-A"
   },
   "source": [
    "# **Text Classification Exercise:**\n",
    "# Scenario:\n",
    "You are building a spam email classifier. The dataset consists of emails labeled as spam or non-spam. Train a text classification model to classify the emails into spam and non-spam categories based on their content.\n",
    "# Dataset: Spam Email Classifier\n",
    "| Email Content                                          | Label   |\n",
    "|--------------------------------------------------------|---------|\n",
    "| Congratulations! You've won a free vacation!          | Spam    |\n",
    "| Important notice: Your account password has expired.  | Non-Spam|\n",
    "| Click here to claim your prize money now!             | Spam    |\n",
    "| Reminder: Your subscription renewal is due next week. | Non-Spam|\n",
    "| Get rich quick with this amazing investment opportunity. | Spam |\n",
    "| Your monthly newsletter is now available. Click to read. | Non-Spam |\n",
    "| Discount offer: Save 50% on all purchases today!      | Spam    |\n",
    "| Urgent: Your package delivery has been delayed.       | Non-Spam|\n",
    "| Make money from home with our easy work-from-home jobs.| Spam   |\n",
    "| Thank you for your recent purchase. Here's your receipt.| Non-Spam |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Email Content     Label\n",
      "0       Congratulations! You've won a free vacation!      Spam\n",
      "1  Important notice: Your account password has ex...  Non-Spam\n",
      "2          Click here to claim your prize money now!      Spam\n",
      "3  Reminder: Your subscription renewal is due nex...  Non-Spam\n",
      "4  Get rich quick with this amazing investment op...      Spam\n",
      "5  Your monthly newsletter is now available. Clic...  Non-Spam\n",
      "6   Discount offer: Save 50% on all purchases today!      Spam\n",
      "7    Urgent: Your package delivery has been delayed.  Non-Spam\n",
      "8  Make money from home with our easy work-from-h...      Spam\n",
      "9  Thank you for your recent purchase. Here's you...  Non-Spam\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load 20newsgroups dataset\n",
    "categories = ['Spam', 'Non-Spam']\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary with 'Email Content' and 'Label' columns\n",
    "data = {\n",
    "    'Email Content': [\n",
    "        \"Congratulations! You've won a free vacation!\",\n",
    "        \"Important notice: Your account password has expired.\",\n",
    "        \"Click here to claim your prize money now!\",\n",
    "        \"Reminder: Your subscription renewal is due next week.\",\n",
    "        \"Get rich quick with this amazing investment opportunity.\",\n",
    "        \"Your monthly newsletter is now available. Click to read.\",\n",
    "        \"Discount offer: Save 50% on all purchases today!\",\n",
    "        \"Urgent: Your package delivery has been delayed.\",\n",
    "        \"Make money from home with our easy work-from-home jobs.\",\n",
    "        \"Thank you for your recent purchase. Here's your receipt.\"\n",
    "    ],\n",
    "    'Label': [\n",
    "        'Spam',\n",
    "        'Non-Spam',\n",
    "        'Spam',\n",
    "        'Non-Spam',\n",
    "        'Spam',\n",
    "        'Non-Spam',\n",
    "        'Spam',\n",
    "        'Non-Spam',\n",
    "        'Spam',\n",
    "        'Non-Spam'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Convert the dictionary into a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Non-Spam       1.00      1.00      1.00         1\n",
      "        Spam       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Encode the labels: 'Spam' -> 1, 'Non-Spam' -> 0\n",
    "df['Label'] = df['Label'].map({'Spam': 1, 'Non-Spam': 0})\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df['Email Content']\n",
    "y = df['Label']\n",
    "\n",
    "# Create a TF-IDF vectorizer to transform the text data into feature vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the email content into TF-IDF features\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print classification report for model evaluation\n",
    "print(classification_report(y_test, y_pred, target_names=['Non-Spam', 'Spam']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1Grw_QF2BoY"
   },
   "source": [
    "# **Sentiment Analysis Exercise:**\n",
    "# Scenario:\n",
    "You are analyzing customer feedback for a product. Perform sentiment analysis on the feedback to identify whether each review is positive, negative, or neutral.\n",
    "\n",
    "# Dataset: Customer feedback for a product\n",
    "\n",
    "| Review                                                    | Sentiment |\n",
    "|-----------------------------------------------------------|-----------|\n",
    "| \"The product exceeded my expectations! Highly recommend.\" | Positive  |\n",
    "| \"I'm satisfied with the quality of the product.\"          | Positive  |\n",
    "| \"This product is average, nothing special.\"               | Neutral   |\n",
    "| \"Disappointed with the performance. Would not buy again.\" | Negative  |\n",
    "| \"Great value for the price. Will purchase again.\"         | Positive  |\n",
    "| \"The product arrived damaged. Poor quality control.\"      | Negative  |\n",
    "| \"So-so product. Not worth the money.\"                     | Neutral   |\n",
    "| \"Absolutely love this product! It's a game-changer.\"      | Positive  |\n",
    "| \"The product did not meet my expectations.\"               | Negative  |\n",
    "| \"Excellent customer service. Resolved my issue quickly.\"  | Positive  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review Sentiment\n",
      "0  The product exceeded my expectations! Highly r...  Positive\n",
      "1     I'm satisfied with the quality of the product.  Positive\n",
      "2          This product is average, nothing special.   Neutral\n",
      "3  Disappointed with the performance. Would not b...  Negative\n",
      "4    Great value for the price. Will purchase again.  Positive\n",
      "5  The product arrived damaged. Poor quality cont...  Negative\n",
      "6                So-so product. Not worth the money.   Neutral\n",
      "7  Absolutely love this product! It's a game-chan...  Positive\n",
      "8          The product did not meet my expectations.  Negative\n",
      "9  Excellent customer service. Resolved my issue ...  Positive\n",
      "Accuracy: 0.50\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.00      0.00      0.00         1\n",
      "    Positive       0.50      1.00      0.67         1\n",
      "\n",
      "    accuracy                           0.50         2\n",
      "   macro avg       0.25      0.50      0.33         2\n",
      "weighted avg       0.25      0.50      0.33         2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Data as a dictionary\n",
    "data = {\n",
    "    'Review': [\n",
    "        \"The product exceeded my expectations! Highly recommend.\",\n",
    "        \"I'm satisfied with the quality of the product.\",\n",
    "        \"This product is average, nothing special.\",\n",
    "        \"Disappointed with the performance. Would not buy again.\",\n",
    "        \"Great value for the price. Will purchase again.\",\n",
    "        \"The product arrived damaged. Poor quality control.\",\n",
    "        \"So-so product. Not worth the money.\",\n",
    "        \"Absolutely love this product! It's a game-changer.\",\n",
    "        \"The product did not meet my expectations.\",\n",
    "        \"Excellent customer service. Resolved my issue quickly.\"\n",
    "    ],\n",
    "    'Sentiment': [\n",
    "        'Positive',\n",
    "        'Positive',\n",
    "        'Neutral',\n",
    "        'Negative',\n",
    "        'Positive',\n",
    "        'Negative',\n",
    "        'Neutral',\n",
    "        'Positive',\n",
    "        'Negative',\n",
    "        'Positive'\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df)\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(df['Review'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Text preprocessing and feature extraction (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "train_features = vectorizer.fit_transform(train_texts)\n",
    "test_features = vectorizer.transform(test_texts)\n",
    "\n",
    "# Model training (Logistic Regression)\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(train_features, train_labels)\n",
    "\n",
    "# Model evaluation\n",
    "predictions = model.predict(test_features)\n",
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
